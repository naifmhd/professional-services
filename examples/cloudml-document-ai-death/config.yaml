# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# Config file for Document AI patents example training and prediction pipelines.

# The project the Document AI pipeline will be built and hosted in.
# TODO(michaelsherman): Make sure no line is past 80 characters (per bash style)
pipeline_project:
  project_id: "death-certificate" # TODO(user): Set to your Project ID.
  region: "us-central1" # Currently (Aug 2019) only "us-central1" supported.
  demo_sample_data: "gs://testing_death_certificates" # TODO(user): GCS location of sample patents to process for demo
  demo_dataset_id: "extracted_death_details" # TODO(user): BQ dataset to store information collected for demo

# Parameters for creation of a service account with get_service_acct.sh.
service_acct:
  creator_user_id: "naifmhd@gmail.com" # TODO(user): Set to your GCP login email.
  acct_name: "death-service-acct"
  acct_description: "Service-account-for-death-demo"
  acct_display_name: "Death-Demo-Service-Account"
  key_path: "$HOME/keys/death-demo-service-acct.json"

# TODO(michaelsherman): Each of these should just be a BQ proj.dataset.table, they should
#   be as independent as possible.
# TODO(michaelsherman): Fields should be specified for all models, like NER is.
pdp_project:
  project_id: "death-certificate"
  dataset_id: "labeled_certificates"
  bucket_name: "sample-certificates"
  image_table_id: "extracted_data"
  objdetect_table_id: "figures"
  text_table_id: "invention_types"
  ner_table_id: "extracted_data"
model_imgclassifier:
  model_id: # 'TODO(user): replace with model id when training is complete
  demo_table_id: "document_classification"
model_objdetect:
  model_id: # 'TODO(user): replace with model id when training is complete
  demo_table_id: "object_detection"
model_textclassifier:
  model_id: # 'TODO(user): replace with model id when training is complete
  demo_table_id: "subject_classification"
model_ner:
  model_id: "TEN6225219332188667904" # 'TODO(user): replace with model id when training is complete
  demo_table_id: "ner_results"
  fields_to_extract:
    - field_name: "gcs_path"
    - field_name: "name"
    - field_name: "address"
    - field_name: "age"
    - field_name: "location"
    - field_name: "time"
    - field_name: "death_date"
    - field_name: "prayer_date"
    - field_name: "contact_number"
